{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":118070,"databundleVersionId":14135400,"sourceType":"competition"},{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239467,"modelId":222398}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_train.csv\")\nprint(train_df.columns)\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install -q transformers accelerate datasets peft safetensors scikit-learn\n\nimport os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any, Optional\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    PreTrainedTokenizerBase,\n)\n\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_DIR = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\nTRAIN_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_train.csv\"\nVAL_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_val.csv\"\nTEST_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_test.csv\"\nSAMPLE_SUB = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/sample_submission.csv\"\n\nOUTPUT_DIR = \"./gemma_lora_output\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EMOTIONS = [\"fear\", \"happy\", \"surprise\", \"sad\", \"anger\", \"disgust\"]\n\nlabel2id = {lab: i for i, lab in enumerate(EMOTIONS)}\nid2label = {i: lab for lab, i in label2id.items()}\n\ndef build_prompt(sentence: str, language: str) -> str:\n    # Prompt instructing the model to produce only the emotion token (no extra text)\n    # We include language to help cross-lingual signals.\n    prompt = (\n        f\"Sentence: {sentence}\\n\"\n        f\"Language: {language}\\n\"\n        f\"Question: What is the emotion expressed in the sentence? Answer with one word from [{', '.join(EMOTIONS)}].\\n\"\n        f\"Answer:\"\n    )\n    return prompt\n\ndef build_prompt_and_target(sentence: str, language: str, emotion: str) -> str:\n    # we'll append \" {emotion}\" as the label target after the prompt.\n    prompt = build_prompt(sentence, language)\n    target = \" \" + emotion  # leading space so tokenizer likely makes it a separate token\n    return prompt, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EmotionCausalDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizerBase, max_length: int = 256, is_train: bool = True):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        sentence = str(row[\"Sentence\"])\n        language = str(row[\"language\"]) if \"language\" in row else \"\"\n        if self.is_train:\n            emotion = str(row[\"emotion\"])\n            prompt, target = build_prompt_and_target(sentence, language, emotion)\n            # Tokenize\n            prompt_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n            target_ids = self.tokenizer(target, add_special_tokens=False)[\"input_ids\"]\n            input_ids = prompt_ids + target_ids\n            # We only want loss on the target tokens -> labels: -100 for prompt tokens\n            labels = [-100] * len(prompt_ids) + target_ids\n        else:\n            # For eval/test we only pass prompt (model will generate)\n            prompt = build_prompt(sentence, language)\n            input_ids = self.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n            labels = None\n\n        if len(input_ids) > self.max_length:\n            # truncate from prompt side (keep target)\n            # ensure target tokens kept by truncating beginning if necessary\n            input_ids = input_ids[-self.max_length:]\n            if labels is not None:\n                labels = labels[-self.max_length:]\n\n        item = {\"input_ids\": torch.tensor(input_ids, dtype=torch.long)}\n        if labels is not None:\n            item[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n        return item\n\n@dataclass\nclass DataCollatorForCausalLMWithPadding:\n    tokenizer: PreTrainedTokenizerBase\n    padding_side: str = \"right\"\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        input_ids = [b[\"input_ids\"] for b in batch]\n        labels = [b.get(\"labels\", None) for b in batch]\n        # pad input_ids\n        padded = self.tokenizer.pad({\"input_ids\": input_ids},\n                                    padding=True,\n                                    return_tensors=\"pt\")\n        if any(l is not None for l in labels):\n            # pad labels, use tokenizer.pad with \"labels\" via same method\n            labels_to_pad = [l if l is not None else torch.tensor([], dtype=torch.long) for l in labels]\n            # convert to list of python lists for tokenizer.pad\n            labels_lists = [l.tolist() if l.numel() > 0 else [] for l in labels_to_pad]\n            padded_labels = self.tokenizer.pad({\"input_ids\": labels_lists},\n                                               padding=True,\n                                               return_tensors=\"pt\")[\"input_ids\"]\n            # replace padding token ids in labels with -100\n            padded_labels[padded_labels == self.tokenizer.pad_token_id] = -100\n            batch_out = {\"input_ids\": padded[\"input_ids\"], \"attention_mask\": padded[\"attention_mask\"], \"labels\": padded_labels}\n        else:\n            batch_out = {\"input_ids\": padded[\"input_ids\"], \"attention_mask\": padded[\"attention_mask\"]}\n        return batch_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\nval_df = pd.read_csv(VAL_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\nprint(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n# Ensure tokenizer has pad token\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# Load as causal LM\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_DIR,\n    trust_remote_code=False,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    low_cpu_mem_usage=True,\n)\n\n# If tokenizer added tokens, resize model embeddings\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Prepare for LoRA (PEFT)\nlora_r = 8\nlora_alpha = 32\nlora_dropout = 0.1\n\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] , # common for causal models, safe to include\n    lora_dropout=lora_dropout,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Prepare model\ntry:\n    model = prepare_model_for_kbit_training(model)\nexcept Exception:\n    pass\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()  # verify LoRA params are trainable\n\nif device == \"cuda\":\n    model = model.to(\"cuda\")\n\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs with DataParallel\")\n    model = torch.nn.DataParallel(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LEN = 256\ntrain_dataset = EmotionCausalDataset(train_df, tokenizer, max_length=MAX_LEN, is_train=True)\nval_dataset = EmotionCausalDataset(val_df, tokenizer, max_length=MAX_LEN, is_train=True)  # val has labels\ntest_dataset = EmotionCausalDataset(test_df, tokenizer, max_length=MAX_LEN, is_train=False)\n\ndata_collator = DataCollatorForCausalLMWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_emotion_from_generation(pred_text: str) -> str:\n    # Normalize generation and pick the first of the known emotions that appears at start or as first token.\n    gen = pred_text.strip().splitlines()[0].strip()\n    # take first token-like chunk\n    first_chunk = gen.split()[0] if len(gen.split())>0 else gen\n    first_chunk = first_chunk.strip().strip('.,;:?\"\\'').lower()\n    # try exact match\n    if first_chunk in EMOTIONS:\n        return first_chunk\n    # try prefix matching\n    for lab in EMOTIONS:\n        if gen.lower().startswith(lab):\n            return lab\n    # fallback: if any label token appears inside\n    for lab in EMOTIONS:\n        if lab in gen.lower():\n            return lab\n    # otherwise return 'unknown' -> treat as wrong\n    return \"unknown\"\n\ndef compute_metrics_for_trainer(eval_preds) -> Dict[str, float]:\n    # Not used directly by Trainer.generate workflow.\n    return {}\n\n# Eval function we can call to produce Macro F1\ndef evaluate_macro_f1(model, dataset, batch_size=8, max_new_tokens=8):\n    model.eval()\n    preds = []\n    labels = []\n    dl = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n    gen_kwargs = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False}\n    for batch in dl:\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch[\"attention_mask\"].to(model.device)\n        with torch.no_grad():\n            generated = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n        # generated: batch x seqlen (includes prompt + generated tokens)\n        for i, gen_ids in enumerate(generated):\n            # decode and strip prompt part: we will decode entire generation and then extract generated suffix\n            decoded = tokenizer.decode(gen_ids, skip_special_tokens=True)\n            input_len = input_ids.shape[1] if isinstance(input_ids, torch.Tensor) else len(batch[\"input_ids\"][i])\n            gen_tokens = gen_ids[input_len:].cpu().numpy().tolist()\n            if len(gen_tokens) == 0:\n                gen_text = \"\"\n            else:\n                gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n            pred_label = predict_emotion_from_generation(gen_text)\n            preds.append(pred_label)\n\n    # Pull true labels from dataset in order\n    for i in range(len(dataset)):\n        # dataset returns item with labels tensor where only target tokens are not -100\n        item = dataset[i]\n        if \"labels\" not in item or item[\"labels\"] is None:\n            labels.append(\"unknown\")\n            continue\n        lab_ids = item[\"labels\"].numpy().tolist()\n        # find positions not -100\n        meaningful = [x for x in lab_ids if x != -100]\n        if len(meaningful)==0:\n            labels.append(\"unknown\")\n        else:\n            # decode meaningful ids to string\n            true_text = tokenizer.decode(meaningful, skip_special_tokens=True)\n            true_label = true_text.strip().split()[0].lower()\n            if true_label in EMOTIONS:\n                labels.append(true_label)\n            else:\n                # fallback: find any emotion present\n                found = None\n                for lab in EMOTIONS:\n                    if lab in true_text.lower():\n                        found = lab; break\n                labels.append(found if found else \"unknown\")\n    \n    # Ensure same length\n    preds = preds[:len(labels)]\n    # convert to numeric for f1_score (map unknown -> some index that'll be treated as wrong)\n    y_true = [label2id[l] if l in label2id else -1 for l in labels]\n    y_pred = [label2id[l] if l in label2id else -1 for l in preds]\n\n    # Build binary per class\n    f1s = []\n    for lab in EMOTIONS:\n        lab_id = label2id[lab]\n        y_t = [1 if y==lab_id else 0 for y in y_true]\n        y_p = [1 if y==lab_id else 0 for y in y_pred]\n        f1 = f1_score(y_t, y_p, zero_division=0)\n        f1s.append(f1)\n    macro_f1 = float(np.mean(f1s))\n    per_class = {lab: float(f1s[i]) for i, lab in enumerate(EMOTIONS)}\n    return {\"macro_f1\": macro_f1, **per_class, \"n_examples\": len(labels)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=2,  # lower if OOM\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,  # to simulate larger batch size\n    num_train_epochs=20,\n    learning_rate=2e-4,\n    \n    fp16=True,\n    fp16_full_eval=True,\n    \n    logging_steps=50,\n    eval_strategy=\"no\",  # we will call eval ourselves\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    remove_unused_columns=False,\n    report_to=\"none\",\n    optim=\"adamw_torch\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=None,\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== STARTING TRAINING ===\")\ntrainer.train()\nprint(\"=== TRAINING FINISHED ===\")\n\n\nomodel = model.module if hasattr(model, \"module\") else model\nmodel = model.module if hasattr(model, \"module\") else model\n\n# Save peft adapters & tokenizer\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_DIR = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\nTRAIN_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_train.csv\"\nVAL_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_val.csv\"\nTEST_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_test.csv\"\nSAMPLE_SUB = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/sample_submission.csv\"\n\nOUTPUT_DIR = \"./gemma_lora_output\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction\n\nimport os\nimport torch\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any, Optional\nfrom transformers import PreTrainedTokenizerBase\n\n\nBASE_MODEL_DIR = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\nADAPTER_DIR = \"./gemma_lora_output\"\nTEST_CSV = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/competition_test.csv\"\nSAMPLE_SUB = \"/kaggle/input/emoti-code-multi-script-emotion-assignment/sample_submission.csv\"\n\n\nBATCH_SIZE = 4\n\nMAX_LEN = 256\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Using device: {DEVICE}\")\n\n\nEMOTIONS = [\"fear\", \"happy\", \"surprise\", \"sad\", \"anger\", \"disgust\"]\n\ndef build_prompt(sentence: str, language: str) -> str:\n    prompt = (\n        f\"Sentence: {sentence}\\n\"\n        f\"Language: {language}\\n\"\n        f\"Question: What is the emotion expressed in the sentence? Answer with one word from [{', '.join(EMOTIONS)}].\\n\"\n        f\"Answer:\"\n    )\n    return prompt\n\ndef predict_emotion_from_generation(pred_text: str) -> str:\n    gen = pred_text.strip().splitlines()[0].strip()\n    first_chunk = gen.split()[0] if len(gen.split()) > 0 else gen\n    first_chunk = first_chunk.strip().strip('.,;:?\"\\'').lower()\n\n    if first_chunk in EMOTIONS:\n        return first_chunk\n    for lab in EMOTIONS:\n        if gen.lower().startswith(lab):\n            return lab\n    for lab in EMOTIONS:\n        if lab in gen.lower():\n            return lab\n    return \"happy\"\n\n\n# Load Model and Tokenizer\nprint(\"Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_DIR,\n    trust_remote_code=False,\n    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n    low_cpu_mem_usage=True,\n)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n\nprint(\"Resizing model token embeddings to match tokenizer...\")\nbase_model.resize_token_embeddings(len(tokenizer))\n\nif base_model.config.pad_token_id is None:\n    base_model.config.pad_token_id = tokenizer.pad_token_id\n\nprint(\"Loading PEFT model and merging adapters...\")\nmodel = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\nmodel = model.merge_and_unload()\nmodel.to(DEVICE)\nmodel.eval()\nprint(\"Model loaded and ready for inference.\")\n\n\n\n# Dataset and Dataloader for Inference\nclass EmotionInferenceDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizerBase, max_length: int):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        sentence = str(row[\"Sentence\"])\n        language = str(row.get(\"language\", \"\"))\n        prompt = build_prompt(sentence, language)\n        \n        tokenized = self.tokenizer(\n            prompt, \n            add_special_tokens=False,\n            truncation=True,\n            max_length=self.max_length,\n        )\n        item = {\"input_ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long)}\n        return item\n\n@dataclass\nclass InferenceDataCollator:\n    tokenizer: PreTrainedTokenizerBase\n    padding_side: str = \"left\"\n\n    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        self.tokenizer.padding_side = self.padding_side\n        input_ids = [b[\"input_ids\"] for b in batch]\n        \n        padded = self.tokenizer.pad(\n            {\"input_ids\": input_ids},\n            padding=True,\n            return_tensors=\"pt\"\n        )\n        return padded\n\n\ntest_df = pd.read_csv(TEST_CSV)\ninference_dataset = EmotionInferenceDataset(test_df, tokenizer, max_length=MAX_LEN)\ninference_collator = InferenceDataCollator(tokenizer=tokenizer)\ninference_dataloader = DataLoader(\n    inference_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=inference_collator,\n    shuffle=False\n)\n\n\n# Run Inference\npredictions = []\ngen_kwargs = {\"max_new_tokens\": 8, \"do_sample\": False}\n\nprint(f\"Starting inference on {len(test_df)} examples...\")\nfor batch in tqdm(inference_dataloader):\n    input_ids = batch[\"input_ids\"].to(DEVICE)\n    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n    \n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            **gen_kwargs\n        )\n        \n    for i, gen_ids in enumerate(generated_ids):\n        input_len = len(input_ids[i])\n        gen_tokens = gen_ids[input_len:]\n        \n        if len(gen_tokens) == 0:\n            gen_text = \"\"\n        else:\n            gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n            \n        pred_label = predict_emotion_from_generation(gen_text)\n        predictions.append(pred_label)\n\nprint(\"Inference complete.\")\n\n\n# Submission File\nsubmission_df = pd.read_csv(SAMPLE_SUB)\nsubmission_df['emotion'] = predictions\n\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' has been created successfully!\")\nprint(submission_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}